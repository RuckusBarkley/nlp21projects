# BART version of closed-book chinese-commonsense QA
This is a BART version of sequence-to-sequence pretrained model for commom sense QA in a closed-book setup, based on [PyTorch](https://pytorch.org/) and [Huggingface's Transformers](https://github.com/huggingface/transformers).

The model is a sequence-to-sequence model that takes a question as an input and outputs the answer, without reading any external resource (e.g. passages, knowledge-graph). 

The model is based on BART-base-chinese. Please refer to [CPT: A Pre-Trained Unbalanced Transformer for Both Chinese Language Understanding and Generation](https://arxiv.org/abs/2109.05729) learn more about BART-chinese.



## Requirement

This code is test on Python 3.9.7 64-bit.

Install Pytorch and Transformers:
```
pip install torch==1.10.0+cu113
pip install transformers==4.12.5
```

Dataset:
The original dataset was crawled from [小知识大全](https://www.sheup.net/info_tiku_1.php) and filtered manually.  
To get train.json, validate.json, test.json in the dataset folder, run `preprocess.py`.


## Training
```
bash src/train.sh
```
It will use ddp training. Modify it if you like.
```
train_bs=40
test_bs=100
python -m torch.distributed.launch --nproc_per_node=3 cli.py --do_train --output_dir out/nq-bart-closed-qa \
        --train_file dataset/train.json \
        --predict_file dataset/dev.json \
        --train_batch_size ${train_bs} \
        --predict_batch_size ${test_bs} \
        --append_another_bos \
        --eval_period 500
```
## Inference

```
bash src/inference.sh
```
Modify it if you like.
```
test_bs=100
python cli.py --do_predict --output_dir out/nq-bart-closed-qa \
        --predict_file src/dataset/dev.json \
        --predict_batch_size ${test_bs} \
        --append_another_bos --prefix dev_
python cli.py --do_predict --output_dir out/nq-bart-closed-qa \
        --predict_file src/dataset/test.json \
        --predict_batch_size ${test_bs} \
        --append_another_bos --prefix test_
```

## Contact
For any question, please post Github issue.
